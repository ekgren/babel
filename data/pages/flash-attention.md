---
lang: en
---
# [FlashAttention](https://github.com/Dao-AILab/flash-attention#flashattention)

**FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness**  
Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, Christopher Ré  
Paper: [https://arxiv.org/abs/2205.14135](https://arxiv.org/abs/2205.14135)